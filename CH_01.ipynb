{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtNvcNYlTa6p6e1WUhKAQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushanr/ml-notes-n-projects/blob/main/CH_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Machine Learning Landscape"
      ],
      "metadata": {
        "id": "b8UafU4ykMju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercises\n",
        "\n",
        "---\n",
        "\n",
        "1. How would you define ML?\n",
        "\n",
        "  * The science of teaching computers to learn from past experience without explicitly programming them.\n",
        "\n",
        "  * A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T as measured by P improves with experience E. \n",
        "\n",
        "2. Four types of problems it excels at?\n",
        "\n",
        "  * Problems for which existing solutions require a lot of fine-tuning or long lists of rules\n",
        "\n",
        "  * Complex problems for which using a traditional approach yields no good solution\n",
        "\n",
        "  * Fluctuating environments, where using ML allows systems to adapt to data\n",
        "\n",
        "  * Getting insights about complex problems and large amounts of data\n",
        "\n",
        "3. What is a labeled training set?\n",
        "\n",
        "  * A set of data which inclueds the desired solutions as labels\n",
        "\n",
        "4. What are the two most common supervised tasks?\n",
        "\n",
        "  * Classification and Regression\n",
        "\n",
        "5. Name four common unsupervised tasks.\n",
        "\n",
        "  * Clustering\n",
        "  * Anomaly/novelty detection\n",
        "  * Visualization and dimensionality reduction\n",
        "  * Association rule learning\n",
        "\n",
        "6. What type of ML algorithm would you use to allow a robot to walk in various unknown terrains?\n",
        "\n",
        "  * An online Reinforcement Learing algorithm would be more suited towards adapting to the changes in an unknown terrain in the shortest possible time.\n",
        "\n",
        "7. What type of algorthm would you use to segment your customers into multiple groups?\n",
        "\n",
        "  * An unsupervised clustering algorithm would be ideal here.\n",
        "\n",
        "8. Would you frame the problem of spam detection as a supervised learning or unsupervised learning problem?\n",
        "\n",
        "  * Spam detection would be a supervised learning problem, because, initially, the algorithm needs to be taught to distinguish between spam/ham using labeled training sets. \n",
        "\n",
        "9. What is an online learning system?\n",
        "\n",
        "  * A system where training is performed by incrementally feeding it data instances sequentially, so the system can learn new data on the fly.\n",
        "\n",
        "10. What is out-of-core learning?\n",
        "\n",
        "  * Follows a similiar approach to online learning, except, the training is typicaly performed offline. Large datasets are usually broken down to smaller chunks and fed into a system that cannot contain the entire dataset for training.\n",
        "\n",
        "11. What type of learning algorithm relies on a similarity measure to make predictions?\n",
        "\n",
        "  * An instance-based learning algorithm uses a similarity index to generalize to test data with respect to the similarity of the training set it was trained on.\n",
        "\n",
        "12. What is the difference between a model parameter and a learning algorithms hyperparameter?\n",
        "\n",
        "  * A model can have one or more model parameters that determines what it will predict given a new instance. It can alter how well the model generalizes to new instances after being optimised on a training set. A learning algorithms hyperparameter, does not belong to the model. It influences the learning algorithm itself, on how much regularization to apply to the model parameters to either make the model more or less sensitive to changes/noise in new instances. \n",
        "\n",
        "13. What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?\n",
        "\n",
        "  * Model-based learning algorithms, search for an optimal value for model parameters, such that they generalize well to new instances. \n",
        "\n",
        "  * The common strategy they use is to minimize a cost-function, which gives a measure of how bad the system is at making predictions on the training set and a penalty for model complexity if the model is regularized.\n",
        "\n",
        "  * They make predictions by extrapolating the new instances using the previously optimized model parameters from the learning algorithm.\n",
        "\n",
        "14. What are the major challenges in ML?\n",
        "\n",
        "  * Lack of data\n",
        "  * Poor data quality\n",
        "  * Nonrepresentative data\n",
        "  * Uninformative features \n",
        "  * Model complexities leading to overfitting/underfitting the data\n",
        "\n",
        "15. If your model can perform great on training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions?\n",
        "\n",
        "  * If the model performs great on training data but fails to generalize to new instances, means that the model is overfitting the data in the training set. \n",
        "\n",
        "  * Possible solutions include;\n",
        "\n",
        "      * Simplify the model by selecting one with fewer parameters (reduce the DOFs of the model)\n",
        "\n",
        "      * Gather more training data.\n",
        "\n",
        "      * Reduce the noise in the training data\n",
        "\n",
        "      * Reduce the non-relevant features or combine redundant features in training data using dimensonality reduction.\n",
        "\n",
        "16. What is a test set, and why would you want to use it?\n",
        "\n",
        "  * A test is used to estimate the generalization error that a model will make on new instances, before a model is launched into production.\n",
        "\n",
        "17. What is a validation set?\n",
        "\n",
        "  * A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters.\n",
        "\n",
        "18. What is the train-dev set? When do you need it? and how do you use it?\n",
        "\n",
        "  * The train-dev set is a part of the training set that is held out. The model is not trained on it.\n",
        "\n",
        "  * A train-dev set is used when there is a risk of mismatch between the training data and the data used in the validation and test sets.\n",
        "\n",
        "  * The model is trained on the rest of the training set and evaluated on the train-dev set and the validation set. If the model performs well on the training set and the train-dev set but not the validation set, then there probably is a significant data mismatch between the train and validation + test sets. \n",
        "\n",
        "  * This happens because the validation + test sets should contain the data that near-perfectly matches new instances that the model is going to encounter after deployment. But due to difficulties in amassing vast amounts of real data, the data used in the training set maybe mixed in with some synthetic data, which can have some unforeseen variances with the actual data concentrated in the validation + test sets.\n",
        "\n",
        "19. What can go wrong if you tune hyperparameters using the test set?\n",
        "\n",
        "  * It will cause the model to overfit the test set data, and therefore lose generalization over new instances. The measured generalization error on the test set however will seemingly improve as the hyperparameters are tuned to the test set."
      ],
      "metadata": {
        "id": "IWvO5647kSrk"
      }
    }
  ]
}